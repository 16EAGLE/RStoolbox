#' Supervised Classification
#' 
#' Supervised classification both for classification and regression mode based on vector training data (points or polygons). 
#' 
#' @param inputRaster Raster* object. Typically remote sensing imagery, which is to be classified.
#' @param trainingData SpatialPolygonsDataFrame or SpatialPointsDataFrame containing the training locations.  
#' @param responseCol Character giving the column in \code{trainingData}, which contains the response variable. Can be omitted, when \code{trainingData} has only one column.
#' @param nSamples Integer. Number of samples per land cover class.
#' @param areaWeightedSampling logical. If \code{TRUE} scales sample size per polygon area. The bigger the polygon the more samples are taken.
#' @param model Character. Which model to use. See \link[caret]{train} for options. Defaults to randomForest ('rf')
#' @param tuneLength Integer. Number of levels for each tuning parameters that should be generated by \link[caret]{train}. 
#' @param filename path to output file (optional). If \code{NULL}, standard raster handling will apply, i.e. storage either in memory or in the raster temp directory.
#' @param verbose logical. prints progress, statistics and graphics during execution
#' @param predict logical. \code{TRUE} (default) will return a classified map, \code{FALSE} will only train the classifier
#' @param ... further arguments to be passed to caret
#' @return A list containing [[1]] the model, [[2]] the predicted raster and [[3]] the class mapping  
#' @seealso \code{\link[caret]{train}} 
#' @export
superClass <- function(inputRaster, trainingData, responseCol = NULL, nSamples = 100, areaWeightedSampling = TRUE,
        model = "rf", tuneLength = 3, 
        filename = NULL, verbose = FALSE,
        predict = TRUE, overwrite = TRUE, ...) {
    # TODO: polygon based cross-validation
    # TODO: add examples
    # TODO: check applicability of raster:::.intersectExtent 
    
    ## Object types
    if(!inherits(inputRaster, 'Raster')) stop("inputRaster must be a raster object (RasterLayer,RasterBrick or RasterStack)", call.=FALSE)
    if(inherits(trainingData, 'SpatialPolygonsDataFrame')) {
        classType <- "polygons"
    } else {
        if(inherits(trainingData, 'SpatialPointsDataFrame')){
            classType <- "points"
        } else {
            stop("traingData must be a SpatialPolygonsDataFrame or a SpatialPointsDataFrame", call.=FALSE)
        }
    }
    
    ## Attribute column
    if(is.null(responseCol)){
        if(ncol(trainingData) == 1) {
            responseCol <- 1
            message("You did not specify the responseCol argument. \nSince your trainingData only contains one column we assume this is it")
        } else {
            stop(paste("Dont't know which column in trainingData contains the class attribute. \nPlease specify responseCol as one of: ", paste(colnames(trainingData@data),collapse=", ")), call. = FALSE)
        }
    } 
    if(!responseCol %in% colnames(trainingData@data)) 
        stop(paste0("The column ", responseCol, " does not exist in trainingData. \nAvailable columns are: ", colnames(trainingData@data,collapse=", ")), call. = FALSE) 
    
    ## Check projections
    if(!compareCRS(inputRaster, trainingData)) 
        stop("Projection of trainingData does not match inputRaster")
    
    ## Check overlap of vector and raster data	
    if(!gIntersects(as(extent(inputRaster),"SpatialPolygons"), as(extent(trainingData),"SpatialPolygons"))) 
        stop("inputRaster and trainingData do not overlap")
    
    ## Calculate area weighted number of samples per polygon
    ## we'll end up with n > nSamples, but make sure to sample each polygon at least once
    if(classType == "polygons"){
        if (areaWeightedSampling){
            if(is.projected(trainingData)){
                trainingData[["area"]] <- gArea(trainingData, byid = TRUE)
            } else {
                trainingData[["area"]] <- areaPolygon(trainingData)		
            }
        } else {
            trainingData[["area"]] <- 1
        }
        
        ## Calculate optimal nSamples per class
        trainingData@data[["order"]] <- 1:nrow(trainingData) 		
        weights <- ddply(trainingData@data, .variables = responseCol, .fun = here(mutate), nSamplesClass = ceiling(nSamples * area / sum(area)))
        trainingData@data <- weights[order(weights$order),]
        
        ## Get random coordinates within polygons
        xy  <- lapply(seq_along(trainingData), function(i_poly){	
                    pts <- spsample(trainingData[i_poly, ], type = "random", n = trainingData@data[i_poly,"nSamplesClass"], iter = 20) 
                })
        xy <- do.call("rbind", xy)
    } else {
        xy <- trainingData
    }
    
    ## What's happening
    mode <- if(is.numeric(trainingData[[responseCol]])) "regression" else "classification"
    
    ## Extract response and predictors and combine in final training set
    if(verbose) message("Begin extract")
    dataSet <- data.frame(
            response = if(classType == "polygons") over(x = xy, y = trainingData)[[responseCol]] else trainingData[[responseCol]],
            extract(inputRaster, xy, cellnumbers = TRUE))
    
    ## Discard duplicate cells
    dubs 	<- duplicated(dataSet[,"cells"])
    dataSet <- dataSet[!dubs,]
    dataSet <- dataSet[,colnames(dataSet) != "cells"]
   

    ## Unique classes
    if(mode == "classification"){   
        if(!is.factor(dataSet$response)) dataSet$response <- as.factor(dataSet$response)
        classes 	 <- unique(dataSet$response)
        classMapping <- data.frame(classID = as.numeric(classes), class = levels(classes))
    }
   
    ### Display, verbose only
    if(verbose) {
        plot(inputRaster,1)
        plot(trainingData, add = T)
        points(xy[!dubs,], pch = 3, cex = 0.5, col = dataSet$response)
    }	
    
    ## Meaningless predictors
    uniqueVals  <- apply(dataSet, 2, function(x){length(unique(x))}) == 1
    if(uniqueVals[1]) stop("Response (responseCol in trainingData) contains only one value. Classification doesn't make sense in this case.")
    if(any(uniqueVals)) {
        warning( "Samples from", paste0(colnames(dataSet)[uniqueVals], collapse = ", "), " contain only one value. 
                        The variable will be omitted from model training.")
        dataSet <- dataSet[, !uniqueVals, drop=FALSE]
    }
    
    ## TRAIN ######################### 
    if(verbose) message("Starting to calculate random forest model") 
    modelFit 	 <- train(response ~ ., data = dataSet, method = model, tuneLength = tuneLength,  trControl = trainControl(method = "cv"))
    
    
    ## PREDICT ######################### 
    progress <- "none"
    if(verbose) { 
        message("Starting spatial predict")
        progress <- "text"
    }
    
    ## Don't know whether we need this, who would be crazy enough to do more than 255 classes...
    performance <- getTrainPerf(modelFit)
    dataType <- NULL
    if(mode == "classification") {
        dataType <- if(length(classes) < 255) "INT1U" else "INT2U"
        performance <- list(performance, confusionMatrix(modelFit, norm = "average"))     
    } 
    
    if(is.null(filename)){
        spatPred <- predict(inputRaster, modelFit, progress = progress, datatype = dataType, overwrite = overwrite)
    } else {
        spatPred <- predict(inputRaster, modelFit, filename = filename, progress = progress, datatype = dataType, overwrite = overwrite)
    }
    
    ## Print summary stats
    if(verbose){
        message(paste0(paste0(rep("*",20), collapse = "")," Classification summary " ,paste0(rep("*",20), collapse = "")))
        print(modelFit)
        print(performance)
    }
    
    out <- list(model = modelFit, performance = performance, map = spatPred)
    if(mode == "classification") out <- c(out, classMapping = classMapping) 
    return(out)
}

